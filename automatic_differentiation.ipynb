{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "dy/dscalar:  6.0\n",
      "(3, 2)\n",
      "dictionary  {'w': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[  1.5121956,  -4.5140023],\n",
      "       [  3.0243912,  -9.028005 ],\n",
      "       [  4.5365868, -13.542007 ]], dtype=float32)>, 'b': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 1.5121956, -4.5140023], dtype=float32)>}\n",
      "lists:  [<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[  1.5121956,  -4.5140023],\n",
      "       [  3.0243912,  -9.028005 ],\n",
      "       [  4.5365868, -13.542007 ]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 1.5121956, -4.5140023], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# intro to tf.GradientTape -> inputs are tf.Variable and not tf.Tensor\n",
    "# tf.GradientTape.gradient(var, wrt can be list, dict, ... ) -> o/p in same form\n",
    "\n",
    "# scalar variable input\n",
    "scalar = tf.Variable(3.0)\n",
    "\n",
    "# tensor rank 1 input\n",
    "w = tf.Variable(tf.random.normal((3,2)), dtype = tf.float32, name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1,2,3]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = scalar**2\n",
    "    y1 = x@w+b\n",
    "    loss = tf.reduce_mean(y1**2)\n",
    "    \n",
    "print(y1.dtype)    \n",
    "print(\"dy/dscalar: \",tape.gradient(y,scalar).numpy())\n",
    "\n",
    "# lets do dy_dw and dy_db. Pass them as list and dictionary\n",
    "lists = tape.gradient(loss,[w,b])\n",
    "dictionary     = tape.gradient(loss, {'w':w, 'b':b})\n",
    "\n",
    "\n",
    "print(w.shape)\n",
    "assert (dy_dw.shape==w.shape), (w.shape,\" not compatible with dw shape \", dy_dw.shape)    \n",
    "print(\"dictionary \", dictionary)\n",
    "print(\"lists: \",lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[3.348121 0.      ]], shape=(1, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 3.348121,  0.      ],\n",
       "        [ 6.696242,  0.      ],\n",
       "        [10.044363,  0.      ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.348121, 0.      ], dtype=float32)>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[3.348121 0.      ]], shape=(1, 2), dtype=float32)\n",
      "layer.trainable_variables:  [<tf.Variable 'dense_16/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-0.09771031,  0.4348526 ],\n",
      "       [ 0.72272885, -0.6991186 ],\n",
      "       [ 0.6667912 , -0.8456247 ]], dtype=float32)>, <tf.Variable 'dense_16/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n",
      "dense_16/kernel:0, shape: (3, 2)\n",
      "dense_16/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Gradinets wrt model\n",
    "# Generally, we want gradients wrt to trainable variables of our model\n",
    "# So its common to collect trainable variables (tf.Variable) in a tf.Module (or its derived classes layers.Layer, keras.Model)\n",
    "# which can be assessed by Module.trainable_variables\n",
    "\n",
    "# 2 nodes in one layer. One input of three feautres -> w1,w2,w3.\n",
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1,2,3]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape0:\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "grad = tape0.gradient(loss, layer.trainable_variables)  \n",
    "print(tape0.gradient(loss,y))\n",
    "\n",
    "grad\n",
    "print(y)\n",
    "print(\"layer.trainable_variables: \",layer.trainable_variables)\n",
    "for var,g in zip(layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# controlling what tape watches. tape will watch only tf.Variable(value, trainable=True)\n",
    "# It can not watch tf.Variable+tf.Tensor==tf.Tensor. however, you can force it to watch by tape.watch(tf.Tensor)\n",
    "\n",
    "x = tf.constant(3.0) # --->>Tensor\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "    \n",
    "print(tape.gradient(y,x))    \n",
    "\n",
    "# Customization tf.GradientTape(persistent=True, watch_accessed_variables=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.75\n"
     ]
    }
   ],
   "source": [
    "# Multiple targets gradient\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "x0 = tf.linspace(-10.0, 10.0, 200+1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "    y = x * [3., 4.]\n",
    "    if len(x0)>0:\n",
    "        tape.watch(x0)\n",
    "        ys = tf.nn.sigmoid(x0)\n",
    "    else:\n",
    "        ys = None\n",
    "\n",
    "# Following is dy0_dx + dy1_dx + dy_dx     \n",
    "print(tape.gradient({'y0': y0, 'y1': y1, 'y': y}, x).numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacks:\n",
    "# x = tf.Variable(...)\n",
    "# x+1-> Tensor -> NOT watch by default\n",
    "# x.assign_add(1) -> Variable\n",
    "# int and string are not differentiable -> can lead to None\n",
    "# if you want Zeros instead of None-> tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
